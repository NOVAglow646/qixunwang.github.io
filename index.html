
<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="icon" href="files/favicon.ico" type="image/x-icon">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Yifei Wang (PKU)</title>
  <meta name="description" content="">

  <link rel="stylesheet" href="main.css">
  <link rel="canonical" href="yifeiwang.github.io">

  <!-- <style>
    a { color: #FF0000; } /* CSS link color */
  </style> -->
</head>

<body data-new-gr-c-s-check-loaded="14.1029.0" data-gr-ext-installed="">

  <header class="site-header" role="banner">
    <div class="wrapper navigation-wrapper ">
      <div class="navigation-links">
        <span class="site-title">Yifei  Wang çŽ‹ä¸€é£›</span>

      </div>
    </div>
  </header>

  <main class="page-content" aria-label="Content">
    <div class="wrapper">

      <!-- Intro -->
      <article class="post">
        <div class="post-content">
          <img src="files/avatar.JPG" class="profile-picture">

          <p>I am a third-year Ph.D. student at <a href="http://www.math.pku.edu.cn">School of Mathematical Sciences</a>, <a href="https://www.pku.edu.cn">Peking University</a>. I am lucky to work at  <a href="https://zero-lab-pku.github.io/">ZERO Lab</a> and co-advised by <a href="https://yisenwang.github.io/">Prof. Yisen Wang</a>, <a href="http://english.math.pku.edu.cn/peoplefaculty/64.html">Prof. Jiansheng Yang</a>, and <a href="https://zhouchenlin.github.io/">Prof. Zhouchen Lin</a>. I obtained my bachelor's degree from <a href="http://www.math.pku.edu.cn">School of Mathematical Sciences</a> and a double bachelor's degree from <a href="https://www.phil.pku.edu.cn/"> Department of Philosophy</a> at <a href="https://www.pku.edu.cn">Peking University</a>. 
            I was born and raised up at the foot of <a href="https://en.wikipedia.org/wiki/Yellow_River">the Yellow River</a> in Shandong, China.
          </p>
          <p>
            As a method-driven researcher, 
            I enjoy discovering new perspectives for understanding and improving Machine Learning algorithms. I mainly work on the following areas and explore their intersections: <br> 
            <span> ðŸŽ‚&nbsp Self-Supervised Learning <br> </span>
            <span> ðŸ”¨&nbsp Adversarial Machine Learning <br></span>
            <span> ðŸŽ²&nbsp Probabilistic Generative Models<br> </span>
            <!-- <span> ðŸŽ²&nbsp Graph Neural Networks and Diffusion Process </span> -->
          </p>
        </div>

          <div class="post-content">
          <p> Contact: yifei_wang at pku.edu.cn (preferred) / 
            <!-- <span style="color:darkslateblue">Contact: yifei_wang AT pku.edu.cn</span> /  -->
            <a href="https://github.com/yifeiwang77">Github</i></span></a> / 
            <!-- <a href="files/wechat_QR.jpg"><span><i style="font-size:20px" class="fa">&#xf1d7;</i></span></a> / -->
            <a href="https://twitter.com/pkuwangyifei">Twitter</a>            
          </p>
          <p> Profile: 
            <a href="files/cv-yifeiwang-2022-02.pdf"><span> Curriculum Vitae (CV)</span></a> /
            <a href="https://openreview.net/profile?id=~Yifei_Wang1"><span>OpenReview</span></a> /
            <a href="https://scholar.google.com/citations?hl=en&user=-CLy6YsAAAAJ"><span>Google Scholar</span></a>
            <!-- <a href="https://scholar.google.com/citations?hl=en&user=-CLy6YsAAAAJ"><span style="color: blue">G</span><span style="color:red">o</span><span style="color:orange">o</span><span style="color:blue">g</span><span style="color:green">l</span><span style="color:red">e</span> <span style="color:grey">Scholar</span></a> -->
          </p>
          <!-- <p>  -->

        </div>
      </article>
    </div>
    <div class="wrapper">
      <!-- Research -->
      <article class="post">
        <header class="post-header">
          <h1 class="post-title">Research</h1>
          <span class="footnote">(* marks equal contribution)</span>
        </header>

        <div class="post-content">
          <ul class="publications">

            <li class="article">
              <span class="title">
              Chaos is a Ladder: A New Theoretical Understanding of Contrastive Learning <br> via Augmentation Overlap <i class='no-italics' >ðŸŽ‚</i>
              </span>
              <span class="authors"><strong>Yifei Wang*</strong>, Qi Zhang*, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
              <span class="journal-info">International Conference on Learning Representations <strong>(ICLR 2022)</strong></span>
              <span class="year">2022</span>
              <span class="links">
                <a href="http://arxiv.org/pdf/2203.13457">PDF</a> |
                <a href="https://github.com/zhangq327/ARC">Code</a> |
                <a href="files/slides/ICLR2022_overlap.pdf">Slides</a>
              </span>
            </li>

            <li class="article">
              <span class="title">
                A Unified Contrastive Energy-based Model for Understanding the Generative Ability  <br> of Adversarial Training <i class='no-italics'>ðŸŽ‚ ðŸ”¨ ðŸŽ²</i>
              </span>
              <span class="authors"><strong>Yifei Wang</strong>, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
              <span class="journal-info">International Conference on Learning Representations <strong>(ICLR 2022)</strong></span>
              <span class="year">2022</span>
              <span class="links">
                <a href="http://arxiv.org/pdf/2203.13455">PDF</a> |
                <a href="files/slides/ICLR2022_CEM.pdf">Slides</a>
              </span>
            </li>

            <li class="article">
              <span class="title">
                Fooling Adversarial Training with Inducing Noise <i class='no-italics'>ðŸ”¨</i>
              </span>
              <span class="authors">Zhirui Wang*, <strong>Yifei Wang*</strong>, Yisen Wang</span>
              <span class="journal-info">arXiv preprint arXiv:2111.10130</span>
              <span class="year">2021</span>
              <span class="links">
                <a href="https://arxiv.org/pdf/2111.10130">PDF</a>
              </span>
              <!-- <span class="intro">Adversarial training is NOT the cure to data poisoning and it can also be fooled. The key is to inject an inducing noise that misleads the models to the hell.</span> -->
            </li>
            
              <li class="article">
              <span class="title">
              Residual Relaxation for Multi-view Representation Learning <i class='no-italics'>ðŸŽ‚</i>
              </span>
              <span class="authors"><strong>Yifei Wang</strong>, Zhengyang Geng, Feng Jiang, Chuming Li, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
              <span class="journal-info">Advances in Neural Information Processing Systems <strong>(NeurIPS 2021)</strong></span>
              <span class="year">2021</span>
              <span class="links">
                <a href="https://arxiv.org/pdf/2110.15348">PDF</a> |
                <a href="files/slides/NeurIPS2021_Prelax_slides.pdf">Slides</a> |
                <a href="https://mp.weixin.qq.com/s/AT9kWTNiImwS-Ns-zM5pCw">Blog </a>
              </span>
              <!-- <span class="intro" hidden="hidden">Do we really need exact alignment of different views in contrastive learning? We show this could be 1) problematic and 2) resolved through a residual relaxation mechanism.</span> -->
            </li>

            <li class="article">
              <span class="title">
                Dissecting the Diffusion Process in Linear Graph Convolutional Networks <i class='no-italics'>ðŸŽ‚</i>
              </span>
              <span class="authors"><strong>Yifei Wang</strong>, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
              <span class="journal-info">Advances in Neural Information Processing Systems <strong>(NeurIPS 2021)</strong></span>
              <span class="year">2021</span>
              <span class="links">
                <a href="https://arxiv.org/pdf/2102.10739">PDF</a> |
                <a href="https://github.com/yifeiwang77/DGC">Code</a> |
                <a href="files/slides/NeurIPS2021_DGC_slides.pdf">Slides</a> |
                <a href="https://mp.weixin.qq.com/s/H5GJnsc3F-qFAUPuZBJ4gA">Blog </a>
              </span>
              <!-- <span class="intro">A properly designed linear GCN (from a <strong>continuous</strong> perspective) is on par with SOTA nonlinear GCNs while being 100x faster => <strong>Unsupervised linear features</strong> can be astonishingly effective.</span> -->
            </li>

            <li class="article">
              <span class="title">
              Reparameterized Sampling for Generative Adversarial Networks <i class='no-italics'>ðŸ”¨ ðŸŽ²</i>
              </span>
              <span class="oral">(Best Paper & Best Student Paper)</span>              
              <span class="authors"><strong>Yifei Wang</strong>, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
              <span class="journal-info">European Conference on Machine Learning <strong>(ECML-PKDD 2021)</strong>
                </span>
              <span class="year">2021</span>
              <span class="links">
                <a href="https://arxiv.org/pdf/2107.00352">PDF</a> |
                <a href="https://github.com/yifeiwang77/repgan">Code</a> |
                <a href="files/slides/ECML2021_REPGAN_slides.pdf">Slides</a> |
                <a href="https://mp.weixin.qq.com/s/Ah43Tqhn0CL2kLxhI_nieQ">Media </a> |
                <a href="https://www.bilibili.com/video/BV1sL4y167gj">Talk </a> |
                <a href="https://2021.ecmlpkdd.org/index.html@p=2148.html">Award</a>
              </span>
              <!-- <span class="intro">Efficient high-dimensional MCMC by reparameterizing Markov transitions into the <strong>low-dimensional latent space </strong> through implicit models (GANs). </span> -->
            </li>

            <li class="article">
              <span class="title">
              Demystifying Adversarial Training via A Unified Probabilistic Framework <i class='no-italics'>ðŸŽ‚ ðŸ”¨ ðŸŽ²</i>
              </span>
              <span class="oral">(Silver Best Paper)</span>
              <span class="authors"><strong>Yifei Wang</strong>, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
              <span class="journal-info">AML Workshop of International Conference on Machine Learning <strong>(ICML-W 2021)</strong></span>
              <span class="year">2021</span>
              <span class="links">
                <a href="https://openreview.net/pdf?id=U0TCTe68s41">PDF</a> |
                <!-- <a href="files/posters/ICML_w2021_CEM_poster.pdf">Poster</a> | -->
                <a href="https://mp.weixin.qq.com/s/E99rIA0lZ6aFeAlYe2Znrw">Blog </a> |
                <a href="https://advml-workshop.github.io/icml2021/">Award</a> | 
                <a href="http://arxiv.org/pdf/2203.13455">Update: conference version @ ICLR 2022</a>
              </span>
              <!-- <span class="intro">Adversarial training implicitly learns a contrastive energy-based model.</span>               -->
            </li>            
            
            <li class="article">
              <span class="title">
              Train Once, and Decode as You Like <i class='no-italics'>ðŸŽ²</i>
              </span>
              <span class="authors">Chao Tian, <strong>Yifei Wang</strong>, Hao Cheng, Yijiang Lian, Zhihua Zhang</span>
              <span class="journal-info">International Committee on Computational Linguistics <strong>(COLING 2020)</strong> </span>
              <span class="year">2020</span>
              <span class="links">
                <a href="https://www.aclweb.org/anthology/2020.coling-main.25.pdf">PDF</a> 
              </span>
              <!-- <span class="intro">Train an autoregressive language model with random ordering masks, and you can decode with whatever order (forward or backward) & whatever decoding steps <strong> (full-, semi-, or non- autoregressive)</strong>. </span> -->
            </li>

            <li class="article">
              <span class="title">
              Decoder-free Robustness Disentanglement without (Additional) Supervision <i class='no-italics'>ðŸ”¨</i>
              </span>
              <span class="authors"><strong>Yifei Wang</strong>, Peng Dan, Furui Liu, Zhenguo Li, Zhitang Chen, Jiansheng Yang</span>
              <span class="journal-info">arXiv preprint arXiv:2007.01356	 </span>
              <span class="year">2020</span>
              <span class="links">
                <a href="https://arxiv.org/pdf/2007.01356">PDF</a> 
              </span>
              <!-- <span class="intro"> Disentangling and preserving both robust and sensitive (non-robust) features in an end-to-end fashion. </span> -->
            </li>
          </ul>
        </div>
        <!-- <span class="footnote">* denotes equal contribution, often meaning multiple authors contributed to coding and running experiments.</span> -->
      </article>
    </div>
    <div class="wrapper">
      <article class="post">
        <header class="post-header">
        <h1 class="post-title">Academic Services</h1>
        </header>
        <div class="post-content">
        Reviewer / PC member of ICLR 2022; ICML 2022; NeurIPS 2021, 2022; ACL ARR 2021, 2022; ECML-PKDD 2022.
      </div>
    </div>
    
    <div class="wrapper">
      <article class="post">
        <header class="post-header">
        <h1 class="post-title">Awards</h1>
        </header>
        <div class="post-content">
          <a href="https://2021.ecmlpkdd.org/index.html@p=2148.html"><strong>Best (Student) Machine Learning Paper Award </strong></a> <i>(first authorship)</i>, ECML-PKDD, 2021.<br>
          <a href="https://advml-workshop.github.io/icml2021/"><strong>Silver Best Paper Award</strong></a> <i>(first authorship)</i>, ICML workshop, 2021.<br>
        <strong>China National Scholarship</strong>, 2021. <br>
        <strong>Scientific Research Award</strong>, Peking University, 2021.
      </div>
    </div>

  <!-- <div class="wrapper">
    <article class="post">
      <header class="post-header">
      <h1 class="post-title">Teaching</h1>
      </header>
      <div class="post-content">
      2022 Spring, TA in <strong>Advances in Machine Learning</strong>, instructed by Prof. Yisen Wang. <br>
      2021 Spring, TA in <strong>Trustworthy Machine Learning</strong>, instructed by Prof. Yisen Wang. <br>
      2018 Spring, TA in <strong>Optimization in Machine Learning</strong>, instructed by Prof. Zhouchen Lin. <br>
      2017 Fall, TA in <strong>Machine Learning</strong>, instructed by Prof. Tong Lin. <br>
    </div>
  </div>
</main> -->
  

  
  <footer class="site-footer">

    <div class="wrapper">

      <div class="footer-col-wrapper">
        <div class="footer-col">

          <!-- <i>Let others boast of the pages they have written; I'm proud of the ones I've read. -- Jorge Luis Borges</i><br> -->
          <!-- <i> Error (â€“ belief in the ideal â€“) is not blindness, error is cowardice. -- Friedrich Nietzsche </i> -->
          <!-- Life is not what one lived, but what one remembers and how one remembers it in order to recount it. -- Gabriel GarcÃ­a MÃ¡rquez. -->
          <ul class="contact-list">
            <li>
              Yifei Wang
            </li>
            <li>Peking University</li>
          </ul>
        </div>
        <div class="footer-col">
        </div>
      </div>
    </div>


  </footer>
